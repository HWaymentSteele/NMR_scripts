{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9c060b47-d83a-45ab-a907-06277c4c7959",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sys, os\n",
    "from glob import glob\n",
    "\n",
    "# for plotting\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def read_PINT(file_dir, T):\n",
    "    ''' Reads in files from PINT.\n",
    "    Inputs:\n",
    "    file_dir: path to directory containing *.out files from PINT.\n",
    "    T: CPMG delay (in seconds)\n",
    "    \n",
    "    Outputs: \n",
    "    dataframe: dataframe with one row per peak dataset (assi)\n",
    "    '''\n",
    "    \n",
    "    fils = sorted(glob(file_dir+'*out'))\n",
    "\n",
    "    output = []\n",
    "\n",
    "    for fil in fils:\n",
    "        d = np.loadtxt(fil)\n",
    "        # sort by first column to get ncyc smallest to largest\n",
    "        d = d[d[:, 0].argsort()]\n",
    "\n",
    "        ncyc, intensity, intensity_err, volume, volume_err = d[:,0], d[:,1], d[:,2], d[:,3], d[:,4]\n",
    "        field = ncyc/T\n",
    "        \n",
    "        assi = os.path.basename(fil).split('.')[0]\n",
    "        \n",
    "        output.append({'assi': assi, 'fields_inc_ref':field, 'intensity': intensity, 'intensity_err': intensity_err,\n",
    "                       'volume': volume, 'volume_err': volume_err})\n",
    "        \n",
    "    dataframe = pd.DataFrame.from_records(output)      \n",
    "    return dataframe\n",
    "\n",
    "def check_has_duplicates(row):\n",
    "    '''\n",
    "    Check dataset has duplicates\n",
    "    '''\n",
    "    tmp = pd.DataFrame({k: row[k] for k in ['intensity','fields_inc_ref']})\n",
    "\n",
    "    #get subset with duplicates\n",
    "    tmp['n_duplicates'] = tmp.groupby('fields_inc_ref')['intensity'].transform('nunique')\n",
    "    dup_subset = tmp.loc[tmp.n_duplicates>1]\n",
    "    \n",
    "    if len(dup_subset) > 1:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "    \n",
    "def get_largest_error(row,xaxis='ncyc', meas='volume',scale=False):\n",
    "    '''\n",
    "    Calculates error using max. Mean Absolute Deviation over duplicate sets\n",
    "    Then selects which error to use based on which is larger, error from noise or error from duplicates\n",
    "    \n",
    "    Inputs:\n",
    "    row: row of dataframe\n",
    "    meas: input to use: volume, intensity, R2eff\n",
    "    \n",
    "    Outputs:\n",
    "    Intensity_err_from_dup: vector of calculated error values on intensities\n",
    "    Vol_err_from_dup: vector of calculated error values on volumes\n",
    "    '''\n",
    "    \n",
    "    #convert row to its own dataframe\n",
    "    tmp = pd.DataFrame({k: row[k] for k in [meas,meas+'_err',xaxis]})\n",
    "\n",
    "    #get subset with duplicates\n",
    "    tmp['n_duplicates'] = tmp.groupby(xaxis)[meas].transform('nunique')\n",
    "    dup_subset = tmp.loc[tmp.n_duplicates>1]\n",
    "    \n",
    "    if len(dup_subset)==0:\n",
    "        raise RuntimeError('this data does not appear to have duplicates.')\n",
    "\n",
    "    dup_subset[meas+'_mean_at_arr'] = dup_subset.groupby(xaxis)[meas].transform('mean')\n",
    "    \n",
    "    if scale:\n",
    "        dup_subset[meas+'_abs_dev_scaled'] = dup_subset.apply(lambda row: np.abs(row[meas] - row[meas+'_mean_at_arr'])/row[meas], axis=1)\n",
    "        tmp[meas+'_MAD'] = tmp.apply(lambda row: row[meas] * np.max(dup_subset[meas+'_abs_dev_scaled']), axis=1)\n",
    "    else:\n",
    "        dup_subset[meas+'_abs_dev'] = dup_subset.apply(lambda row: np.abs(row[meas] - row[meas+'_mean_at_arr']), axis=1)\n",
    "        tmp[meas+'_MAD'] = np.max(dup_subset[meas+'_abs_dev'])\n",
    "\n",
    "    # select which error to use\n",
    "    bigger_error = ['duplicates','noise'][np.argmax([tmp[meas+'_MAD'].mean(), tmp[meas+'_err'].mean()])]\n",
    "\n",
    "    return tmp[meas+'_MAD'].values, bigger_error    \n",
    "        \n",
    "def calculate_R2eff(row, T, value='volume'):\n",
    "    '''\n",
    "    Calculates R2_eff either based on PINT-calculated peak intensity or volume.\n",
    "    \n",
    "    Inputs:\n",
    "    row: row of dataframe\n",
    "    T: delay (in s)\n",
    "    value: column to use to calculate (`intensity` or `volume`)\n",
    "    \n",
    "    Outputs:\n",
    "    R2eff: vector of calculated R2eff values\n",
    "    R2eff_err: propagated R2eff error\n",
    "    ncyc: values of ncyc to plot (not including references of 0)\n",
    "    '''\n",
    "    \n",
    "    ref_inds = np.where(row['fields_inc_ref']==0)\n",
    "    arr_inds = np.where(row['fields_inc_ref']!=0)\n",
    "        \n",
    "    I0 = np.mean(row[value][ref_inds])\n",
    "    I0_err = np.sqrt(np.mean(np.square(row[value+'_err'][ref_inds])))\n",
    "    \n",
    "    I= row[value][arr_inds]\n",
    "    I_err = row[value+'_err'][arr_inds]\n",
    "    \n",
    "    R2eff = np.log(I0/I)/T\n",
    "    R2eff_err = np.sqrt((I0_err/I0)**2 + (I_err/I)**2) * R2eff\n",
    "    \n",
    "    return R2eff, R2eff_err, row['fields_inc_ref'][arr_inds]\n",
    "\n",
    "def plot_peak(row):\n",
    "    \n",
    "    os.makedirs('output_plots',exist_ok=True)\n",
    "    if row['larger_err']=='duplicates':\n",
    "        plt.errorbar(row['ncyc'], row['R2eff'],yerr=row['R2eff_dup_err'],capsize=2,fmt='.')\n",
    "    else:\n",
    "        plt.errorbar(row['ncyc'], row['R2eff'],yerr=row['R2eff_err'],capsize=2,fmt='.')\n",
    "    plt.xlabel(r'$\\nu_{CPMG}$ (Hz)')\n",
    "    plt.ylabel(r'$R_{2,eff}$')\n",
    "    plt.title('%s\\nerr method: %s' % (row['assi'], row['larger_err']))\n",
    "    plt.savefig('output_plots/%s.pdf' % row['assi'],bbox_inches='tight')\n",
    "    plt.close()\n",
    "    \n",
    "def plot_both_errs(row):\n",
    "    os.makedirs('output_plots/both_errs',exist_ok=True)\n",
    "    plt.errorbar(row['ncyc'], row['R2eff'],yerr=row['R2eff_err'],capsize=2,fmt='.',label='noise err')\n",
    "    plt.errorbar(row['ncyc'], row['R2eff'],yerr=row['R2eff_dup_err'],capsize=2,fmt='.',label='duplicate err')\n",
    "    plt.legend()\n",
    "    plt.xlabel(r'$\\nu_{CPMG}$ (Hz)')\n",
    "    plt.ylabel(r'$R_{2,eff}$')\n",
    "    plt.title('%s\\nerr method: %s' % (row['assi'], row['larger_err']))\n",
    "    plt.savefig('output_plots/both_errs/%s.pdf' % row['assi'],bbox_inches='tight')\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f57e1fca-ef74-430f-9359-3e7497c118c0",
   "metadata": {},
   "source": [
    "processCPMG.ipynb\n",
    "\n",
    "H Wayment-Steele, last updated 1 may 2025\n",
    "\n",
    "Usage: set path to output and delay time for CPMG, then run cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "357ab18e-afb3-4dd6-a684-f03b99047dec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 39 peak datasets, calculating R2eff...\n",
      "Dataset has duplicates. Identifying largest error estimate...\n",
      "Creating plots in output_plots/*pdf ...\n",
      "Creating plots with both error types to compare in output_plots/both_errs/*pdf ...\n",
      "\n",
      "Raw data is in output_plots/raw_data.json.zip\n",
      "Reload this in python with `df = pd.read_json('raw_data.json.zip')`\n",
      "\n",
      "How many peaks used duplicates vs. noise?\n",
      "larger_err\n",
      "duplicates    29\n",
      "noise         10\n"
     ]
    }
   ],
   "source": [
    "# SET PATH TO OUTPUT AND DELAY\n",
    "T=0.04\n",
    "dat = read_PINT('out_NU/',T=T)\n",
    "\n",
    "print('Found %d peak datasets, calculating R2eff...' % len(dat))\n",
    "\n",
    "#first calculate R2eff and propagate R2eff_err from noise.\n",
    "dat[['R2eff','R2eff_err','ncyc']] = dat.apply(lambda row: calculate_R2eff(row,T=T, value='volume'), axis=1,result_type='expand')\n",
    "\n",
    "has_duplicate = check_has_duplicates(dat.iloc[0])\n",
    "if has_duplicate:\n",
    "    print('Dataset has duplicates. Identifying largest error estimate...')\n",
    "    #then determine error from R2eff duplicates.\n",
    "    dat[['R2eff_dup_err','larger_err']] = dat.apply(lambda row: get_largest_error(row,meas='R2eff',xaxis='ncyc'), axis=1,result_type='expand')\n",
    "else:\n",
    "    print('no duplicates found.')\n",
    "    \n",
    "#create plots\n",
    "print('Creating plots in output_plots/*pdf ...')\n",
    "dat.apply(lambda row: plot_peak(row), axis=1)\n",
    "print('Creating plots with both error types to compare in output_plots/both_errs/*pdf ...')\n",
    "dat.apply(lambda row: plot_both_errs(row), axis=1)\n",
    "\n",
    "dat.to_json('output_plots/raw_data.json.zip')\n",
    "\n",
    "print('')\n",
    "print('Raw data is in output_plots/raw_data.json.zip')\n",
    "print(\"Reload this in python with `df = pd.read_json('raw_data.json.zip')`\")\n",
    "print('')\n",
    "print('How many peaks used duplicates vs. noise?')\n",
    "print(dat.groupby('larger_err').size().to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c81a506-c8ab-43a7-83af-688c9df64a34",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
